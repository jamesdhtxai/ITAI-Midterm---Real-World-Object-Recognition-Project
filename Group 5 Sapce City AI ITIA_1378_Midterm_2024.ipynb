{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Computer Vision Midterm Assignment\n",
        "## Introduction\n",
        "Welcome to your Computer Vision midterm project! Here, you'll get hands-on experience building an image recognition model using Convolutional Neural Networks and transfer learning.\n"
      ],
      "metadata": {
        "id": "3Dow3z3e28v8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install Necessary Libraries:"
      ],
      "metadata": {
        "id": "QskG_sM13fWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "uu5AcXJL2oT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.applications import VGG16, ResNet50, MobileNetV2  # Choose a pre-trained model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Additional libraries for data loading (if using a custom dataset)\n",
        "# from skimage.io import imread  # Example for loading images\n"
      ],
      "metadata": {
        "id": "MdVjWkYsPxXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Selection and Loading**\n",
        "\n",
        "* **Choose Your Dataset**\n",
        "   * **Standard Datasets:** CIFAR-10, CIFAR-100, or a suitable subset of ImageNet are good starting points. You can use built-in functions to load them.\n",
        "   * **Custom Dataset:** If you propose a custom dataset, ensure it has sufficient images per class, good quality, and accurate labeling. You'll need to upload it to Colab.\n",
        "   * **Select your dataset and uncomment the appropriate loading code.**\n",
        "   * **If you are using a custom dataset, make sure you have uploaded it to Colab and adjust the file path.**"
      ],
      "metadata": {
        "id": "qaTFINe_2oT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select your dataset\n",
        "# from keras.datasets import cifar10 # Or cifar100, or a suitable ImageNet loader\n",
        "\n",
        "\n",
        "# *** Dataset Loading - Uncomment the lines for your chosen dataset ***\n",
        "\n",
        "# Option 1: CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Option 2: CIFAR-100\n",
        "# (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# Option 3: Custom Dataset\n",
        "# x_train, y_train = load_custom_data('path/to/your/training/data')\n",
        "# x_test, y_test = load_custom_data('path/to/your/testing/data')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ZH0NU0mX2oT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Markdown Cell: Exploratory Data Analysis (EDA)**\n",
        "\n",
        "* **Instructions:**\n",
        "    * Visualize a few random images from your dataset to understand its content and overall quality.\n",
        "    * Check the shape of your data to confirm the number of images and their dimensions."
      ],
      "metadata": {
        "id": "pf_1y3xg2oUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert codode here\n",
        "# Insert code here to display a few sample images from the dataset\n",
        "## Display sample images\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(x_train[i])\n",
        "plt.show()\n",
        "#\n",
        "print('Training data shape:', x_train.shape)\n",
        "print('Training labels shape:', y_train.shape)\n",
        "print('Test data shape:', x_test.shape)\n",
        "print('Test labels shape:', y_test.shape)\n",
        "\n",
        "# Explore class distribution (if using a standard dataset)\n",
        "from collections import Counter\n",
        "print('Class Distribution (Top 10):')\n",
        "print(Counter(np.argmax(y_train, axis=1)).most_common(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "d9DfVj-H2oUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Preprocessing**\n",
        "\n",
        "* **Instructions:**\n",
        "    1. **Normalization:**\n",
        "       * Normalize pixel values (usually to the range of 0-1 or -1 to 1)  \n",
        "    2. **Resizing:**\n",
        "       * Resize images to a consistent size for model input."
      ],
      "metadata": {
        "id": "chCZKMBb2oUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert code here to normalize images\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Resize images if needed (adjust input_shape in model building accordingly)\n",
        "# x_train = tf.image.resize(x_train, (224, 224))  # Example for resizing to 224x224\n",
        "# x_test = tf.image.resize(x_test, (224, 224))\n",
        "\n",
        "# Insert code here to resize images, if needed"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9ZyT_f4v2oUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** Data Augmentation **\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "1. Experiment with Parameters:  The code below has some example data augmentation parameters. Try changing the values within these parameters, or even adding new augmentation techniques! Here's a short guide:\n",
        "\n",
        "* Hint 1: Start with small adjustments to see the effects clearly.\n",
        "* Hint 2: Consider which augmentations make sense for your dataset. Flipping images of letters might be okay, but rotating them too much could make them unreadable!\n",
        "\n",
        "* Explore more: Try adding things like shear_range (for shearing transformations) or zoom_range (for random zooming).\n",
        "\n",
        "2. Visualize the Effects: After setting up your ImageDataGenerator, add a few lines of code to display some randomly augmented images from your dataset. This will help you see how your chosen parameters change the images.\n",
        "* Hint: Use a small sample of images so it's easy to compare the originals with the augmented versions."
      ],
      "metadata": {
        "id": "hoXLleLY2oUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "       rotation_range=20,\n",
        "       width_shift_range=0.1,\n",
        "       height_shift_range=0.1,\n",
        "       horizontal_flip=True,\n",
        "       # Add more augmentations if desired\n",
        ")\n",
        "datagen.fit(x_train) # Fit the augmentation parameters to the training data\n"
      ],
      "metadata": {
        "id": "c3F2ZrN0QrFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Building (Transfer Learning)"
      ],
      "metadata": {
        "id": "TH5QEV1iXt04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a pre-trained model suitable for object recognition (VGG16, ResNet50, MobileNetV2 are all options)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=x_train.shape[1:])\n",
        "\n",
        "# Freeze some layers of the pre-trained model (optional)\n",
        "for layer in base_model.layers[:10]:\n",
        "   layer.trainable = False  # Adjust the number of layers to freeze as needed\n",
        "\n",
        "# Add custom top layers\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Adjust num_classes for your dataset\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "XACmoPHiXz49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UTTGKyvULGtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    epochs=15,  # Adjust as needed\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zoI8WqiM2oUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Enhanced Training\n",
        "\n",
        "Implement data augmentation within the training loop.\n",
        "Add callbacks to monitor progress and save the best performing model.\n",
        "Modify the Training Code: If you haven't already, we need to make a few changes to your training loop:\n",
        "\n",
        "1.   Integrate the Data Augmentation: Replace the\n",
        "direct use of x_train with datagen.flow(x_train, y_train, batch_size=32). This will apply your augmentations in real-time during training\n",
        "2.   Use the Validation Set: We already have validation_data=(x_test, y_test).\n",
        "3. Save the Best Model: We're using a ModelCheckpoint callback to automatically save the model if its performance on the validation set improves\n",
        "* Hint: Experiment with different batch sizes as well."
      ],
      "metadata": {
        "id": "uYF3iK5C9Tli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Data Augmentation with ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "       rotation_range=20,\n",
        "       width_shift_range=0.1,\n",
        "       height_shift_range=0.1,\n",
        "       horizontal_flip=True)\n",
        "\n",
        "#  Modify the model fitting to use real-time augmentation\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
        "                    epochs=15,\n",
        "                    validation_data=(x_test, y_test),  # Use the test set for validation\n",
        "                    callbacks=[ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')])\n"
      ],
      "metadata": {
        "id": "NQyHQhLy9gdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing Training Progress\n",
        "\n",
        "Importance of Monitoring: Explain why tracking validation metrics helps identify overfitting or underfitting.\n",
        "\n",
        "*   Plot training and validation accuracy/loss curves.\n"
      ],
      "metadata": {
        "id": "wNkYr3hH91ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation curves\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Plot the loss curves\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "83wjfPBORSx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation on the Test Set\n",
        "\n",
        "Discuss how test set metrics provide the most unbiased assessment of model performance."
      ],
      "metadata": {
        "id": "E3Kt5VXD94F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = load_model('best_model.h5')\n",
        "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "p79iThglZp2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter Tuning\n",
        "\n",
        ">Exploring Learning Rates: In the provided code, we're iterating through different learning rates.\n",
        "* Hint 1: A good starting range for the learning rate is often between 0.01 and 0.0001.\n",
        "* Hint 2: Pay close attention to how quickly the validation loss starts to increase (if it does), which might signal a learning rate that's too high.\n",
        "\n"
      ],
      "metadata": {
        "id": "xwaS7o0S_EJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(learning_rate=0.01):\n",
        "    # ... (Code to build your model, using the learning_rate parameter)\n",
        "    return model\n",
        "\n",
        "# Basic parameter exploration\n",
        "for lr in [0.01, 0.001, 0.0001]:\n",
        "    model = create_model(learning_rate=lr)\n",
        "    # ... (Training the model)\n"
      ],
      "metadata": {
        "id": "UEN5i4f8RaG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Confusion Matrx"
      ],
      "metadata": {
        "id": "WfjlOsFq_dxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "y_pred = best_model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sn.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vgRpZW4-ZyUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Discussion and Further Exploration\n",
        "\n",
        "##Questions to consider:\n",
        "1. How does the choice of pre-trained model (VGG16, ResNet50, etc.) affect the results?\n",
        "2. Analyze the confusion matrix: Are errors more common between certain classes? What might explain this?\n",
        "3. Experiment with different degrees of fine-tuning (freezing more/fewer layers of the pre-trained model).\n",
        "4. If applicable to your dataset, can you collect more data for classes with higher error rates?\n",
        "What are other ways to potentially improve accuracy? (e.g., ensembling models, exploring advanced augmentation strategies, class-weighted training)\n",
        "\n",
        "Sources\n",
        "towardsdatascience.com/build-your-own-deep-learning-classification-model-in-keras-511f647980d6\n",
        "stackoverflow.com/questions/69997327/tensorflow-valueerror-input-0-is-incompatible-with-layer-model-expected-shape\n",
        "www.influxdata.com/blog/time-series-forecasting-with-tensorflow-influxdb/"
      ],
      "metadata": {
        "id": "rC2AmcC2Z2Xx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}